{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "282dcbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import dataset\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "import dataset\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fab073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 3\n",
    "img_size = 244\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "classes = ['empty', 'full']\n",
    "num_classes = len(classes)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "validation_size = .2\n",
    "\n",
    "train_path = './dataset/'\n",
    "test_path = './test/'\n",
    "checkpoint_dir = './models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4da09496",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = [None, img_size, img_size, num_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7bdb3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "Loading empty files (Index: 0)\n",
      "Loading full files (Index: 1)\n",
      "Reading test images\n"
     ]
    }
   ],
   "source": [
    "data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_size)\n",
    "test_images, test_ids = dataset.read_test_set(test_path, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9d72183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t2117\n",
      "- Test-set:\t\t5\n",
      "- Validation-set:\t529\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(test_images)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.valid.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat #, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af3d13",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \n",
    "    x = tf.nn.conv2d(filters, kernel, padding='same', strides=strides)(inputs)\n",
    "    x = tf.nn.batch_normalization()(x)\n",
    "    return relu6(x)\n",
    "\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, s, r=False):\n",
    "    \n",
    "    tchannel = inputs.shape[-1] * t\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "    x = tf.nn.depthwise_conv2d(kernel, strides=(s, s), padding='same')(x)\n",
    "    x = tf.nn.batch_normalization()(x)\n",
    "    x = tf.nn.relu6(x)\n",
    "    \n",
    "    x = tf.nn.conv2d(filters, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.nn.batch_normalization()(x)\n",
    "\n",
    "    if r:\n",
    "        x = x = tf.math.add(x, inputs)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, strides, n):\n",
    "    \n",
    "    x = _bottleneck(inputs, filters, kernel, t, strides)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(x, filters, kernel, t, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "def MobileNetV2(inputs, plot_model=False):\n",
    "    \"\"\"MobileNetv2\n",
    "    This function defines a MobileNetv2 architecture.\n",
    "    # Arguments\n",
    "        input_shape: An integer or tuple/list of 3 integers, shape\n",
    "            of input tensor.\n",
    "        k: Integer, number of classes.\n",
    "        plot_model: Boolean, whether to plot model architecture or not\n",
    "    # Returns\n",
    "        MobileNetv2 model.\n",
    "    \"\"\"\n",
    "\n",
    "#     x = tf.placeholder(\"float\", image_shape)\n",
    "#     inputs = Input(shape=input_shape, name='input')\n",
    "    x = _conv_block(inputs, 32, (3, 3), strides=(2, 2))\n",
    "\n",
    "    x = _inverted_residual_block(x, 16, (3, 3), t=1, strides=1, n=1)\n",
    "    x = _inverted_residual_block(x, 24, (3, 3), t=6, strides=2, n=2)\n",
    "    x = _inverted_residual_block(x, 32, (3, 3), t=6, strides=2, n=3)\n",
    "    x = _inverted_residual_block(x, 64, (3, 3), t=6, strides=2, n=4)\n",
    "    x = _inverted_residual_block(x, 96, (3, 3), t=6, strides=1, n=3)\n",
    "    x = _inverted_residual_block(x, 160, (3, 3), t=6, strides=2, n=3)\n",
    "    x = _inverted_residual_block(x, 320, (3, 3), t=6, strides=1, n=1)\n",
    "\n",
    "    x = _conv_block(x, 1280, (1, 1), strides=(1, 1))\n",
    "#     x = flatten_layer(x)\n",
    "    x = tf.reshape((1, 1, 1280))(x)\n",
    "    x = tf.nn.dropout(0.3, name='Dropout')(x)\n",
    "    x = tf.nn.conv2d(k, (1, 1), padding='same')(x)\n",
    "    output = tf.nn.softmax('softmax', name='final_activation')(x)\n",
    "#     output = tf.reshape((k,), name='output')(x)\n",
    "    \n",
    "#     model = tf.keras.models.Model(inputs, output)\n",
    "#     model.summary()\n",
    "#     if plot_model:\n",
    "#         tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return output\n",
    "\n",
    "def trashcan_net(inputs, plot_model=False):\n",
    "    \n",
    "    x = MobileNetV2(inputs, plot_model=False)\n",
    "    x = tf.nn.avg_pool(ksize=(7, 7))(x)\n",
    "    x = flatten_layer(x)\n",
    "    x = tf.compat.v1.layers.dense(128)(x)\n",
    "    x = tf.nn.relu6(x)\n",
    "    x = tf.nn.dropout(0.5)(x)\n",
    "    x = tf.compat.v1.layers.dense(2)(x)\n",
    "    output = tf.nn.softmax()(x)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "089c9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начинаем эпоху 0\n",
      "[[[[0.07450981 0.08627451 0.10196079]\n",
      "   [0.27450982 0.28627452 0.3019608 ]\n",
      "   [0.7019608  0.70980394 0.7254902 ]\n",
      "   ...\n",
      "   [0.909804   0.74509805 0.5803922 ]\n",
      "   [0.91372555 0.7372549  0.65882355]\n",
      "   [0.41960788 0.26666668 0.37254903]]\n",
      "\n",
      "  [[0.07450981 0.08627451 0.10196079]\n",
      "   [0.24313727 0.25490198 0.27058825]\n",
      "   [0.6862745  0.69803923 0.7137255 ]\n",
      "   ...\n",
      "   [0.73333335 0.5137255  0.25490198]\n",
      "   [0.6313726  0.43921572 0.15294118]\n",
      "   [0.70980394 0.5568628  0.37647063]]\n",
      "\n",
      "  [[0.06666667 0.08235294 0.09411766]\n",
      "   [0.227451   0.2392157  0.2509804 ]\n",
      "   [0.68235296 0.69411767 0.70980394]\n",
      "   ...\n",
      "   [0.89019614 0.6666667  0.3803922 ]\n",
      "   [0.8941177  0.70980394 0.34117648]\n",
      "   [0.7411765  0.6117647  0.28235295]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7803922  0.80392164 0.8313726 ]\n",
      "   [0.53333336 0.5568628  0.58431375]\n",
      "   [0.6        0.62352943 0.6509804 ]\n",
      "   ...\n",
      "   [0.14901961 0.21176472 0.27450982]\n",
      "   [0.15686275 0.20392159 0.27058825]\n",
      "   [0.15294118 0.19215688 0.25882354]]\n",
      "\n",
      "  [[0.48235297 0.5058824  0.53333336]\n",
      "   [0.3137255  0.3372549  0.3647059 ]\n",
      "   [0.6039216  0.627451   0.654902  ]\n",
      "   ...\n",
      "   [0.12156864 0.18039216 0.24313727]\n",
      "   [0.12941177 0.18039216 0.24313727]\n",
      "   [0.14117648 0.18039216 0.24705884]]\n",
      "\n",
      "  [[0.38431376 0.40784317 0.43529415]\n",
      "   [0.49803925 0.52156866 0.54901963]\n",
      "   [0.5803922  0.6039216  0.6313726 ]\n",
      "   ...\n",
      "   [0.10980393 0.16862746 0.23137257]\n",
      "   [0.1137255  0.16470589 0.227451  ]\n",
      "   [0.13333334 0.17254902 0.2392157 ]]]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-8c6f71f425dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrashcan_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Вычислим значение потерь для этого батча\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-aa7b3ee73cd2>\u001b[0m in \u001b[0;36mtrashcan_net\u001b[1;34m(inputs, plot_model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrashcan_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-06e8cc08d8a6>\u001b[0m in \u001b[0;36mMobileNetV2\u001b[1;34m(inputs, plot_model)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#     x = tf.placeholder(\"float\", image_shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#     inputs = Input(shape=input_shape, name='input')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_conv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_inverted_residual_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-06e8cc08d8a6>\u001b[0m in \u001b[0;36m_conv_block\u001b[1;34m(inputs, filters, kernel, strides)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_conv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrelu6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2160\u001b[0m   \"\"\"\n\u001b[0;32m   2161\u001b[0m   \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2162\u001b[1;33m   return conv2d(input,  # pylint: disable=redefined-builtin\n\u001b[0m\u001b[0;32m   2163\u001b[0m                 \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2164\u001b[0m                 \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[0;32m   2258\u001b[0m   \u001b[0mdilations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dilations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2260\u001b[1;33m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2261\u001b[0m   \u001b[1;31m# shape object may lack ndims, e.g., if input is an np.ndarray.  In that case,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2262\u001b[0m   \u001b[1;31m# we fall back to len(shape).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Итерируем по эпохам\n",
    "grad_log = []\n",
    "\n",
    "epochs = 10  # учим немного, т.к. задача посмотреть , что происходит с градиентом\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    print(f'\\nНачинаем эпоху {epoch}') \n",
    "\n",
    "    # Итерируем по батчам в датасете\n",
    "    for step in range(epochs): \n",
    "        \n",
    "        # Откроем GradientTape чтобы записать операции\n",
    "        # выполняемые во время прямого прохода, включающего автодифференцирование\n",
    "        with tf.GradientTape() as tape: \n",
    "            # Запустим прямой проход слоя\n",
    "            \n",
    "            x_batch, y_true_batch, _, cls_batch = data.train.next_batch(step+1)\n",
    "            \n",
    "            print(x_batch)\n",
    "            \n",
    "            preds = trashcan_net(x_batch) \n",
    "\n",
    "            # Вычислим значение потерь для этого батча\n",
    "            loss_value = loss_fn(y_true_batch, preds)\n",
    "\n",
    "            # Используем gradient tape для автоматического извлечения градиентов \n",
    "            # обучаемых переменных относительно потерь\n",
    "            grads = tape.gradient(loss_value, modelg.trainable_weights) \n",
    "            g_g = []\n",
    "\n",
    "            # пишем логи для сохранения значений градиента и веса по одной цепи \n",
    "            for g_s in grads:\n",
    "                # допишем логи значений градиента в зависимости от размера тензора градиента\n",
    "                # if len(g_s.numpy().shape) == 1:\n",
    "                #     g_g.append(g_s.numpy()[0])\n",
    "                if len(g_s.numpy().shape) == 2:\n",
    "                    g_g.append(g_s.numpy()[0, 0]) \n",
    "\n",
    "                \n",
    "        # добавляем текущие логи по слоям к общей записи\n",
    "        grad_log.append(g_g)\n",
    "\n",
    "        # Выполним один шаг градиентного спуска,\n",
    "        # обновив значение переменных минимизирующих потери\n",
    "        optimizer.apply_gradients(zip(grads, modelg.trainable_weights)) \n",
    "\n",
    "        # Пишем лог каждые 200 шагов\n",
    "        if step % 200 == 0:\n",
    "            print(f'Эпоха {epoch + 1}/{epochs}', end='. ')\n",
    "            print(f'Шаг {step}. Лосс на обучении (для одного батча) на шаге: {loss_value}') \n",
    "            print(f'Уже увидели: {(step + 1) * batch_size} примеров')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b07b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef9721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870164f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c7ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f50a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b75ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2f483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20341e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd58c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf76a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518d160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9520f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9ef1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6915eb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'x:0' shape=(?, 178608) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d34e028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 244, 244, 3) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b864dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7f2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e20ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019e4a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Relu:0' shape=(?, 122, 122, 32) dtype=float32>,\n",
       " <tf.Variable 'Variable:0' shape=(3, 3, 3, 32) dtype=float32_ref>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1, weights_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3b980e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de806f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Relu_1:0' shape=(?, 61, 61, 32) dtype=float32>,\n",
       " <tf.Variable 'Variable_2:0' shape=(3, 3, 32, 32) dtype=float32_ref>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2, weights_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8e7bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv3, weights_conv3 = \\\n",
    "    new_conv_layer(input=layer_conv2,\n",
    "                   num_input_channels=num_filters2,\n",
    "                   filter_size=filter_size3,\n",
    "                   num_filters=num_filters3,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d4c44e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Relu_2:0' shape=(?, 31, 31, 64) dtype=float32>,\n",
       " <tf.Variable 'Variable_4:0' shape=(3, 3, 32, 64) dtype=float32_ref>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv3, weights_conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8038d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db37df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Reshape_1:0' shape=(?, 61504) dtype=float32>, 61504)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bcbe99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61504"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96a9be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72f56c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86dfcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "992ddb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_4:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69a9476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d14953dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6804a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5187e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc2, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1450afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3837e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd21c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b49197dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40025c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8f7ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99179907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    # Calculate the accuracy on the training-set.\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce57c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
    "\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, flattened image shape]\n",
    "\n",
    "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
    "#         print(x_batch.size)\n",
    "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "        \n",
    "        feed_dict_validate = {x: x_valid_batch,\n",
    "                              y_true: y_valid_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "        \n",
    "\n",
    "        # Print status at end of each epoch (defined as full pass through training dataset).\n",
    "        if i % int(data.train.num_examples/batch_size) == 0: \n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_validate)\n",
    "            epoch = int(i / int(data.train.num_examples/batch_size))\n",
    "            \n",
    "            print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss)\n",
    "            \n",
    "            if early_stopping:    \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "\n",
    "                if patience == early_stopping:\n",
    "                    break\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "920ac7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_errors(cls_pred, correct):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.valid.images[incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.valid.cls[incorrect]\n",
    "\n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images, cls_true=cls_true, cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b5f885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cls_pred):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = data.valid.cls\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31b661e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_accuracy(show_example_errors=False, show_confusion_matrix=False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.valid.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_test)\n",
    "        \n",
    "        print(data.valid.images[i:j, :].size)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.valid.images[i:j, :].reshape(batch_size, img_size_flat)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.valid.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images,\n",
    "                     y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    cls_true = np.array(data.valid.cls)\n",
    "    cls_pred = np.array([classes[x] for x in cls_pred]) \n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # Plot the confusion matrix, if desired.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76904582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.valid.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a2563e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- Training Accuracy:  68.8%, Validation Accuracy:  59.4%, Validation Loss: 0.811\n",
      "Time elapsed: 0:00:05\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "5715456\n",
      "4465200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4465200 into shape (32,178608)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-e584d4f62691>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint_validation_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_example_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-1bf645f11283>\u001b[0m in \u001b[0;36mprint_validation_accuracy\u001b[1;34m(show_example_errors, show_confusion_matrix)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Get the images from the test-set between index i and j.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size_flat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 4465200 into shape (32,178608)"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1)\n",
    "print_validation_accuracy(show_example_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87141a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 --- Training Accuracy:  81.2%, Validation Accuracy:  71.9%, Validation Loss: 0.619\n",
      "Time elapsed: 0:00:03\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607339e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_accuracy(show_example_errors=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bf02e2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
